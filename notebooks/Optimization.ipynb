{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6c55e03-236f-4528-87e1-3951e667b680",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import numpy as np\n",
    "from huggingface_hub import hf_hub_download\n",
    "import pandas as pd\n",
    "import json\n",
    "from pprint import pprint, pformat\n",
    "from gurobipy import Model, GRB\n",
    "from gurobipy import quicksum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c802677-6f95-4a3b-b540-58ed0d9aa75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load GlotLID Model\n",
    "glot_model_path = hf_hub_download(repo_id=\"cis-lmu/glotlid\", filename=\"model.bin\", cache_dir=None)\n",
    "glot_model = fasttext.load_model(glot_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab97fe7c-1202-4e27-9546-ee8e46c8be47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load OpenLID Model\n",
    "open_model_path = hf_hub_download(repo_id=\"laurievb/OpenLID\", filename=\"model.bin\")\n",
    "open_model = fasttext.load_model(open_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94a81719-dba0-4eff-a29d-aaf727d140d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from copy import deepcopy\n",
    "\n",
    "class MaskLID:\n",
    "    def __init__ (self, model_path, languages=-1):\n",
    "        self.model = fasttext.load_model(model_path)\n",
    "        self.output_matrix = self.model.get_output_matrix()\n",
    "        self.labels = self.model.get_labels()\n",
    "        self.language_indices = self._compute_language_indices(languages) #menyimpan index label/bahasa\n",
    "        self.labels = [self.labels[i] for i in self.language_indices] #mengembalikan label/bahasa untuk seluruh index di self.language_indices\n",
    "\n",
    "    def _compute_language_indices(self, languages): #mengembalikan list index untuk label bahasa yang ingin dipakai\n",
    "        if languages != -1 and isinstance(languages, list): #cek apakah languanges bukan -1 dan benar-benar berupa list\n",
    "                return [self.labels.index(l) for l in set(languages) if l in self.labels] #ini return kalo languanges paramnya bukan -1\n",
    "        return list(range(len(self.labels))) #ini return kalo languanges paramnya = -1\n",
    "\n",
    "    def _softmax(self, x): #x itu array dot product antara vektor bahasa dan rata2 vektor kalimat\n",
    "        exp_x = np.exp(x - np.max(x)) #setiap nilai di array x dikurang dengan nilai max di dalam array itu supaya gak overflow\n",
    "        return exp_x /np.sum(exp_x)\n",
    "\n",
    "    def _normalize_text(self, text): #intinya ini biar input kalimat bersih\n",
    "        replace_by = \" \"\n",
    "        replacement_map = {ord(c): replace_by for c in '\\n_:' + 'â€¢#{|}' + string.digits} #menarik buat dipelajari nanti, ini buat dictionary utuk mapping tags2 aneh dan menggantinya jadi \" \"\n",
    "        text = text.translate(replacement_map)\n",
    "        return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    #ini untuk menemukan bahasa dominan di kalimat itu, makanya k=1 (cuman buat nentuin bahasa apa yg mau di mask)\n",
    "    def predict(self, text, k=1): #inget ini cuman kembaliin top-k prediksi\n",
    "        sentence_vector = self.model.get_sentence_vector(text)\n",
    "        result_vector = np.dot(self.output_matrix, sentence_vector) #logit skor (skor logit tiap bahasa)\n",
    "        softmax_result = self._softmax(result_vector)[self.language_indices] #ambil skor logit hanya untuk bahasa yang dipilih (index bahasa di self.language_indices)\n",
    "        top_k_indices = np.argsort(softmax_result)[-k:][::-1] #ambil k index terakhir (yg terbesar) lalu balik urutannya [::-1] biar dari besar ke kecil\n",
    "        top_k_labels = [self.labels[i] for i in top_k_indices] #ambil label bahasanya\n",
    "        top_k_probs = softmax_result[top_k_indices] #ambil probabilitasnya\n",
    "        return tuple(top_k_labels), top_k_probs\n",
    "\n",
    "    #mirip seperti predicct tapi lebih ke analitik\n",
    "    def compute_v(self, sentence_vector):\n",
    "        result_vector = np.dot(self.output_matrix[self.language_indices, :], sentence_vector)  #hitung logit skor bahasa yang dipilih seperti biasa\n",
    "        return sorted(zip(self.labels, result_vector), key=lambda x:x[1], reverse=True) #Lalu urutkan berdasarkan skor (x[1]), dari tinggi ke rendah\n",
    "\n",
    "    def compute_v_per_word(self, text): #ini intinya buat hitung vector per kata di kalimat, ini harusnya keurut (maksudnya terurut kata di kalimat sesuai kalimatnya) sih di dict_text \n",
    "        text = self._normalize_text(text)\n",
    "        words = self.model.get_line(text)[0] #ambil bagian word/tokennya saja soalnya model.det_line itu mengembalikan [token,label]\n",
    "        words = [w for w in words if w not in ['</s>', '</s>']] #ambil tokennya saja, buang bagian '</s>', '</s>'\n",
    "        subword_ids = [self.model.get_subwords(sw)[1] for sw in words] #ambil id/index sw nya\n",
    "        sentence_vector = [np.sum([self.model.get_input_vector(id) for id in sid], axis=0) for sid in subword_ids] #menghasilkan satu vektor representasi akhir untuk kata itu dengan menambahkan setiap subword vector dari kata itu\n",
    "\n",
    "        #nilai logit tiap kata yang diurtkan dari besar ke kecil\n",
    "        dict_text = {}\n",
    "        for i, word in enumerate(words):\n",
    "            key = f\"{i}_{word}\"\n",
    "            dict_text[key] = {'logits': self.compute_v(sentence_vector[i])} #setiap satu key (index,kata) akan punya skor logit untuk bahasa di self.language_indices\n",
    "\n",
    "        return dict_text\n",
    "\n",
    "    \n",
    "    def mask_label_top_k(self, dict_text, label, top_keep, top_remove): #ini kan tujuan intinya menghasilkan dict_remained sama dict_deleted (yg seharusnya di mask)\n",
    "        dict_remained = deepcopy(dict_text)\n",
    "        dict_deleted = {}\n",
    "\n",
    "        for key, value in dict_text.items(): #jadi ini di iterasi per kata (setiap kata punya nilai logit untuk setiap bahasa)\n",
    "            logits = value['logits'] \n",
    "            labels = [t[0] for t in logits]  \n",
    "            \n",
    "            if label in labels[:top_keep]: #kalau label target (sesuai parameter) ada di top_keep kata itu, simpan kata itu di dict_deleted\n",
    "                dict_deleted[key] = dict_remained[key]\n",
    "\n",
    "            if label in labels[:top_remove]:\n",
    "                dict_remained.pop(key, None)\n",
    "\n",
    "        return dict_remained, dict_deleted\n",
    "\n",
    "    @staticmethod\n",
    "    def get_sizeof(text):\n",
    "        return len(text.encode('utf-8'))\n",
    "\n",
    "    @staticmethod\n",
    "    def custom_sort(word): #ini dipake buat dapetin angka di depan word aja? gatau juga buat apa\n",
    "        match = re.match(r'^(\\d+)_', word) \n",
    "        if match:\n",
    "            return int(match.group(1)) ##Ekstrak angka sebelum underscore (_)/grup pertama\n",
    "        else:\n",
    "            return float('inf') # Return infinity for words without numbers at the beginning\n",
    "\n",
    "    def sum_logits(self, dict_data, label): #ini gatau kenapa dia sum skor logit dari label yang dicari (misalnya '__label__eng_Latn'), dan tambah ke total .\n",
    "        total = 0\n",
    "        \n",
    "        for value in dict_data.values():\n",
    "            logits = value['logits']\n",
    "            labels = [t[0] for t in logits]\n",
    "            if label in labels:\n",
    "                total += logits[labels.index(label)][1]\n",
    "        return total\n",
    "\n",
    "    \n",
    "    def predict_codeswitch(self, text, beta, alpha, min_prob, min_length, max_lambda=1, max_retry=3, alpha_step_increase=5, beta_step_increase=5):\n",
    "\n",
    "        info = {} #untuk menyimpan hasil masking yang sukses\n",
    "        index = 0 #menghitung berapa kali masking berhasil\n",
    "        retry = 0 #menghitung berapa kali masking gagal (tidak memenuhi syarat confidence/panjang)\n",
    "\n",
    "        \n",
    "        #hitung embedding per kata dari text, untuk tiap bahasa yang didukung model\n",
    "        #Output: dictionary berisi representasi vektor kata-kata, dikunci dengan nama seperti 0_Bonjour, 1_i, dll.\n",
    "        dict_data = self.compute_v_per_word(text)\n",
    "\n",
    "\n",
    "        #while akan berlanjut kalo dua kondisi dibawah memenuhi, kalo satu ga memenuhi, ga lanjut\n",
    "        while index < max_lambda and retry < max_retry:\n",
    "            print(f\"\\n--- iteration : {index + retry + 1}---\")\n",
    "            print(f\"Current index: {index}, retry: {retry}\")\n",
    "            print(f\"Current beta: {beta}, Current alpha: {alpha}\")\n",
    "            \n",
    "\n",
    "            #prediksi bahasa untuk text saat ini.\n",
    "            pred = self.predict(text, k=1)\n",
    "            label = pred[0][0] # ambil label bahasa dominan (misal '__label__eng_Latn')\n",
    "            score = pred[1][0]\n",
    "            print(f\"Predicted label (P(c|s)): ({label}, {score:.2f}) from averaging text: {text}\") #label akan jadi target masking: kita ingin lihat apakah masking beberapa kata akan mengubah label ini.\n",
    "            print(f\"Current dict_data (Vs) : {' '.join(x.split('_', 1)[1] for x in dict_data.keys())}\")\n",
    "            #Simpan versi teks sebelum dimask, untuk rollback kalau masking gagal.\n",
    "            prev_text = text\n",
    "\n",
    "            dict_data, dict_masked = self.mask_label_top_k(dict_data, label, beta, alpha)\n",
    "            \n",
    "\n",
    "            masked_text = ' '.join(x.split('_', 1)[1] for x in dict_masked.keys()) #bagian kata-kata yang dianggap kontributor utama terhadap label dominan\n",
    "            print(f\"masked text (words that fall within the beta threshold) : {masked_text}\")\n",
    "            text = ' '.join(x.split('_',1)[1] for x in dict_data.keys()) #bagian sisa yang akan dipakai buat prediksi ulang â†’ untuk lihat apakah label dominan masih bertahan setelah penghapusan sebagian kata\n",
    "            print(f\"text (words remaining, not fall within alpha): {text}\")\n",
    "            \n",
    "            # cek apakah hasil masked_text masih cukup panjang untuk diproses,\n",
    "            # atau ini adalah iterasi pertama (wajib diproses meski teks pendek)\n",
    "            if self.get_sizeof(masked_text) > min_length or index == 0:\n",
    "                print(f\"-------------------------\")\n",
    "                print(f\"if condition when masked text > {min_length} or index = 0\")\n",
    "                print(f\"\\tcheck masked_text length\")\n",
    "                print(f\"\\tmasked_text length: {self.get_sizeof(masked_text)}\")\n",
    "                #lakukan prediksi ulang pada teks yang sudah dimask\n",
    "                #(Ini penting untuk lihat apakah masking tetap mempertahankan label dominan)\n",
    "                temp_pred = self.predict(masked_text) #perhatiin ini ngepredict masked_text (ini bisa buat liat contigues dan sisa kata kayanya)\n",
    "                print(f\"\\tpredicted label from masked text: {temp_pred}\")\n",
    "               \n",
    "                #cek apakah confidence label top-1 tinggi (misalnya > 0.90)\n",
    "                #cek apakah label top-1 == label yang dicari\n",
    "                #iterasi pertama tetap disimpan meskipun confidence-nya kurang\n",
    "                #Jika terpenuhi, artinya: masking tidak merusak label dominan, artinya bisa disimpan.\n",
    "                if (temp_pred[1][0]> min_prob and temp_pred[0][0] == label) or index == 0:\n",
    "                    print(f\"\\tif condition when : masked_text confidence label > {min_prob} AND its label=={label} OR index=0 \")\n",
    "                    info[index]={\n",
    "                        'label':label, #bahasa dominan saat ini \n",
    "                        'text': masked_text, #bagian teks yang dimask\n",
    "                        'text_keys': dict_masked.keys(), #indeks+kata dari kata-kata yang dimask \n",
    "                        'size': self.get_sizeof(masked_text), #ukuran teks yang dimask (berapa banyak katanya)\n",
    "                        'sum_logit': self.sum_logits(dict_masked, label) #ukuran teks yang dimask (berapa banyak katanya)\n",
    "                    }\n",
    "                    print(f\"\\t\\tlook what info has :\")\n",
    "                    print(f\"\\t\\ttext_keys (saving all maksed_text to info):\")\n",
    "                    for key in info[index]['text_keys']:\n",
    "                        print(f\"\\t\\t{key}\")\n",
    "                    print(f\"\\t\\tsum logit: {info[index]['sum_logit']}\")\n",
    "\n",
    "                    index+= 1 #ukuran teks yang dimask (berapa banyak katanya)\n",
    "\n",
    "                #kalau confidence untuk label top-1 kurang dari min_prob, atau\n",
    "                #kalau label hasil masking berubah (tidak sama dengan label awal sebelum masking), atau\n",
    "                #ini bukan iterasi pertama (index != 0)\n",
    "                else:\n",
    "                    print(f\"\\telse condition, when masked text: \\nconfidence for top-1 label is less than min_prob AND masking result label changed\\nOR\\nthis is not the first iteration (index != 0)\")\n",
    "                    text = prev_text #kembalikan teks ke kondisi sebelum masking, artinya: hasil masking tadi dianggap buruk, jadi dibatalkan, dan teks dikembalikan ke versi sebelumnya.\n",
    "                    print(f\"\\t\\tRollback to previous sentence form (without masking)  : {text}\")\n",
    "                    beta += beta_step_increase #di iterasi berikutnya, sistem akan mencoba masking lebih banyak kata (alpha lebih besar)\n",
    "                    alpha += alpha_step_increase #tapi juga menyimpan lebih banyak kata pendukung (beta lebih besar)\n",
    "                    print(f\"\\t\\tincrease beta to {beta} and alpha to {alpha}\")\n",
    "                    retry +=1 #Naikkan jumlah percobaan gagal, ini penting supaya loop tidak jalan terus. Jika retry mencapai max_retry, maka loop akan berhenti.\n",
    "                    \n",
    "            else: \n",
    "                # Kalau hasil masked_text terlalu pendek dan ini bukan iterasi pertama\n",
    "                print(f\"else condition, when : masked_text shorter than {min_length} OR index !=0\")\n",
    "                text = prev_text\n",
    "                print(f\"Rollback to previous sentence form (without masking): {text}\")\n",
    "                beta += beta_step_increase\n",
    "                alpha += alpha_step_increase\n",
    "                print(f\"\\tincrease beta to {beta} and alpha to {alpha}\")\n",
    "                retry += 1\n",
    "                \n",
    "            print(f\"Text remaining to process: {text}\")\n",
    "\n",
    "            if self.get_sizeof(text) < min_length:\n",
    "                print(f\"Remaining text to process is to short, less than {min_length}\")\n",
    "                break\n",
    "\n",
    "            if index == max_lambda or retry == max_retry:\n",
    "                print(f\"Loop stopped. Final status: index = {index}, retry = {retry}, max_lambda = {max_lambda}, max_retry = {max_retry}\")\n",
    "            else:\n",
    "                print(f\"Loop continue. Final status: index = {index}, retry = {retry}, max_lambda = {max_lambda}, max_retry = {max_retry}\")\n",
    "                \n",
    "            \n",
    "\n",
    "        #post-process\n",
    "        post_info = {}\n",
    "        for value in info.values():\n",
    "            key = value['label'] \n",
    "            if key in post_info:\n",
    "                post_info[key].extend(value['text_keys'])\n",
    "            else:\n",
    "                post_info[key] = list(value['text_keys'])\n",
    "    \n",
    "        for key in post_info:\n",
    "            post_info[key] = ' '.join([x.split('_', 1)[1] for x in sorted(set(post_info[key]), key=self.custom_sort)])\n",
    "\n",
    "        return post_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a86b5d0-e042-44e2-ba67-3b0f788eebec",
   "metadata": {},
   "outputs": [],
   "source": [
    "flores_glotlid = ['__label__eng_Latn', '__label__arb_Arab', '__label__rus_Cyrl', '__label__por_Latn', '__label__pol_Latn', '__label__ekk_Latn', '__label__ell_Grek', '__label__slk_Latn', '__label__slv_Latn', '__label__nld_Latn', '__label__lvs_Latn', '__label__hun_Latn', '__label__dan_Latn', '__label__swe_Latn', '__label__lit_Latn', '__label__fin_Latn', '__label__mlt_Latn', '__label__cmn_Hani', '__label__nob_Latn', '__label__kor_Hang', '__label__ind_Latn', '__label__uzn_Latn', '__label__fil_Latn', '__label__ukr_Cyrl', '__label__hin_Deva', '__label__hin_Latn', '__label__afr_Latn', '__label__mar_Deva', '__label__ceb_Latn', '__label__ilo_Latn', '__label__zul_Latn', '__label__heb_Hebr', '__label__xho_Latn', '__label__vie_Latn', '__label__jpn_Jpan', '__label__guj_Gujr', '__label__hrv_Latn', '__label__tur_Latn', '__label__nya_Latn', '__label__tsn_Latn', '__label__sna_Latn', '__label__tso_Latn', '__label__tha_Thai', '__label__spa_Latn', '__label__deu_Latn', '__label__eus_Latn', '__label__bul_Cyrl', '__label__amh_Ethi', '__label__fra_Latn', '__label__ewe_Latn', '__label__mkd_Cyrl', '__label__nso_Latn', '__label__tam_Taml', '__label__lin_Latn', '__label__twi_Latn', '__label__yor_Latn', '__label__als_Latn', '__label__ibo_Latn', '__label__ben_Beng', '__label__ita_Latn', '__label__tpi_Latn', '__label__azj_Latn', '__label__run_Latn', '__label__mya_Mymr', '__label__kin_Latn', '__label__ron_Latn', '__label__ces_Latn', '__label__kat_Geor', '__label__urd_Arab', '__label__zsm_Latn', '__label__pap_Latn', '__label__bem_Latn', '__label__mal_Mlym', '__label__kir_Cyrl', '__label__hye_Armn', '__label__smo_Latn', '__label__sin_Sinh', '__label__fij_Latn', '__label__kan_Knda', '__label__pan_Guru', '__label__hau_Latn', '__label__epo_Latn', '__label__gaz_Latn', '__label__tir_Ethi', '__label__bos_Latn', '__label__srp_Cyrl', '__label__hat_Latn', '__label__pag_Latn', '__label__lua_Latn', '__label__war_Latn', '__label__tel_Telu', '__label__tat_Cyrl', '__label__sag_Latn', '__label__lug_Latn', '__label__tum_Latn', '__label__swh_Latn', '__label__umb_Latn', '__label__som_Latn', '__label__gle_Latn', '__label__kng_Latn', '__label__mos_Latn', '__label__lus_Latn', '__label__khk_Cyrl', '__label__asm_Beng', '__label__tuk_Latn', '__label__quy_Latn', '__label__ayr_Latn', '__label__luo_Latn', '__label__tgk_Cyrl', '__label__cat_Latn', '__label__ssw_Latn', '__label__nno_Latn', '__label__cym_Latn', '__label__kik_Latn', '__label__kmb_Latn', '__label__ory_Orya', '__label__bel_Cyrl', '__label__bho_Deva', '__label__apc_Arab', '__label__bak_Cyrl', '__label__jav_Latn', '__label__yue_Hani', '__label__pbt_Arab', '__label__khm_Khmr', '__label__npi_Deva', '__label__npi_Latn', '__label__gug_Latn', '__label__uig_Arab', '__label__fur_Latn', '__label__kbp_Latn', '__label__hne_Deva', '__label__kam_Latn', '__label__gla_Latn', '__label__kab_Latn', '__label__arz_Arab', '__label__kaz_Cyrl', '__label__mri_Latn', '__label__lim_Latn', '__label__srd_Latn', '__label__sun_Latn', '__label__plt_Latn', '__label__mni_Beng', '__label__isl_Latn', '__label__vec_Latn', '__label__glg_Latn', '__label__scn_Latn', '__label__fao_Latn', '__label__san_Deva', '__label__ltz_Latn', '__label__cjk_Latn', '__label__ast_Latn', '__label__lmo_Latn', '__label__szl_Latn', '__label__oci_Latn', '__label__fon_Latn', '__label__min_Latn', '__label__wol_Latn', '__label__lij_Latn', '__label__ajp_Arab', '__label__snd_Arab', '__label__dik_Latn', '__label__ary_Arab', '__label__lao_Laoo', '__label__ars_Arab', '__label__bjn_Latn', '__label__shn_Mymr', '__label__crh_Latn', '__label__aeb_Arab', '__label__ace_Latn', '__label__ckb_Arab', '__label__dyu_Latn', '__label__ltg_Latn', '__label__kmr_Latn', '__label__ban_Latn', '__label__mai_Deva', '__label__fuv_Latn', '__label__kac_Latn', '__label__taq_Latn', '__label__bam_Latn', '__label__sat_Olck', '__label__tzm_Tfng', '__label__bug_Latn', '__label__dzo_Tibt', '__label__kas_Deva', '__label__fas_Arab', '__label__nus_Latn', '__label__knc_Latn', '__label__mag_Deva', '__label__taq_Tfng', '__label__kas_Arab', '__label__knc_Arab', '__label__bjn_Arab', '__label__ace_Arab', '__label__kea_Latn', '__label__awa_Deva', '__label__acm_Arab', '__label__bod_Tibt', '__label__sot_Latn', '__label__ydd_Hebr', '__label__azb_Arab']\n",
    "\n",
    "#custom model from limited languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d23320fb-c6f0-49bd-866e-ec264cc175dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "try1 = MaskLID(model_path=open_model_path, languages=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a73b1fff-4e9b-4605-86a1-dbd97798b6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "try2 = MaskLID(model_path=glot_model_path, languages=flores_glotlid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "101b8309-50b8-4ec5-9562-d217dc8baef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_te = pd.read_csv('df_te.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0263567-3fbe-47c8-b767-93ebccde1b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t = pd.read_csv('df_turk.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05ee0172-66d0-400d-aa8d-901cd9466b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_te1 = df_te.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35288f34-420e-4ed3-b2d1-00db80a4f38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_te2 = df_te.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6bc9e11f-5d28-42d9-a9b2-4e2d083b105d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t1 = df_t.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0040b57-4689-40a2-94f0-74fc8f4273df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t2 = df_t.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c25a86c6-d83c-4778-af47-d886e7706a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_masklid_predictions(df, assignments, store_column=\"Predicted\"):\n",
    "    em_count = 0\n",
    "    pm_count = 0\n",
    "    fp_count = 0\n",
    "\n",
    "    em_indices = []\n",
    "    pm_indices = []\n",
    "    fp_indices = []\n",
    "\n",
    "    total = len(df)\n",
    "    total_cs = sum(len(gold) > 1 for gold in df[\"True Labels\"])\n",
    "\n",
    "    df[store_column] = None\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        gold = row[\"True Labels\"]\n",
    "        pred = assignments[i]  # list of (word, label)\n",
    "        pred_labels = {label for (_, label) in pred}\n",
    "        df.at[i, store_column] = pred_labels\n",
    "\n",
    "        if not isinstance(gold, set):\n",
    "            gold = set(gold)\n",
    "\n",
    "        if len(gold) > 1:  # CS sentence\n",
    "            if pred_labels == gold:\n",
    "                em_count += 1\n",
    "                pm_count += 1\n",
    "                em_indices.append(i)\n",
    "                pm_indices.append(i)\n",
    "            elif len(pred_labels) == 1 and next(iter(pred_labels)) in gold:\n",
    "                pm_count += 1\n",
    "                pm_indices.append(i)\n",
    "            elif len(pred_labels) > 1:\n",
    "                if len(gold - pred_labels) == 1 :  # includes unexpected label(s)\n",
    "                    fp_count += 1\n",
    "                    fp_indices.append(i)\n",
    "\n",
    "        else:  # Mono sentence\n",
    "            if pred_labels == gold:\n",
    "                em_count += 1\n",
    "                pm_count += 1\n",
    "                em_indices.append(i)\n",
    "                pm_indices.append(i)\n",
    "            elif gold & pred_labels:\n",
    "                pm_count += 1\n",
    "                pm_indices.append(i)\n",
    "            # FP not counted for mono sentences\n",
    "\n",
    "    result = {\n",
    "        \"EM\": em_count,\n",
    "        \"PM\": pm_count,\n",
    "        \"FP\": fp_count,\n",
    "        \"Total\": total,\n",
    "        \"Total_CS\": total_cs,\n",
    "        \"EM_idx\": em_indices,\n",
    "        \"PM_idx\": pm_indices,\n",
    "        \"FP_idx\": fp_indices,\n",
    "    }\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c5d5c5-c133-47af-8d39-ef10cb42b469",
   "metadata": {},
   "source": [
    "OpenLID with Turkish-ENglish Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d65870c5-47c9-4d06-aa19-e547145056de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2025-11-12\n"
     ]
    }
   ],
   "source": [
    "from gurobipy import Model, GRB, quicksum\n",
    "\n",
    "# Konfigurasi\n",
    "min_len = 15  # minimum length of characters for a language to be considered valid\n",
    "alpha = 2     # maximum number of active languages (according to top-alpha)\n",
    "\n",
    "results = []\n",
    "open_assignments_te = []\n",
    "\n",
    "# Looping for each line (sentence) in the dataset\n",
    "for idx, row in df_te1.iterrows():\n",
    "    text = row[\"Text\"]\n",
    "\n",
    "    # Step 1: Calculate the logit scores per word from the FastText model\n",
    "    scores_dict = try1.compute_v_per_word(text)\n",
    "\n",
    "    # Step 2: Take the top-Î± of each word\n",
    "    top_alpha_languages = set()\n",
    "    for word in scores_dict:\n",
    "        sorted_logits = sorted(scores_dict[word]['logits'], key=lambda x: -x[1])\n",
    "        top_alpha = [lang for lang, _ in sorted_logits[:alpha]]\n",
    "        top_alpha_languages.update(top_alpha)\n",
    "\n",
    "    words = list(scores_dict.keys())\n",
    "    languages = list(top_alpha_languages)\n",
    "\n",
    "    # Step 3: Take the logit values â€‹â€‹b_ij (parameters, not variables)\n",
    "    b = {}\n",
    "    for i in words:\n",
    "        for j, score in scores_dict[i]['logits']:\n",
    "            if j in languages:\n",
    "                b[(i, j)] = score\n",
    "\n",
    "    # Step 4: Initialize the Gurobi model\n",
    "    model = Model(\"MaskLID_ILP\")\n",
    "    model.setParam('OutputFlag', 0)\n",
    "\n",
    "    # Main variables\n",
    "    x = {(i, j): model.addVar(vtype=GRB.BINARY, name=f\"x_{i}_{j}\") for (i, j) in b}\n",
    "    y = {j: model.addVar(vtype=GRB.BINARY, name=f\"y_{j}\") for j in languages}\n",
    "    model.update()\n",
    "\n",
    "    # Character length of each word\n",
    "    word_lengths = {w: len(w.split('_', 1)[1]) for w in words}\n",
    "\n",
    "    # Constraint 1: One word only one language label\n",
    "    for i in words:\n",
    "        model.addConstr(quicksum(x[(i, j)] for j in languages if (i, j) in x) == 1)\n",
    "\n",
    "    # Constraint 2: Language j is only active if there is an active x_ij\n",
    "    for j in languages:\n",
    "        model.addConstr(quicksum(x[(i, j)] for i in words if (i, j) in x) <= len(words) * y[j])\n",
    "\n",
    "    # Constraint 3: Total word length per language â‰¥ min_len if y_j is active\n",
    "    for j in languages:\n",
    "        model.addConstr(\n",
    "            quicksum(x[(i, j)] * word_lengths[i] for i in words if (i, j) in x) >= min_len * y[j],\n",
    "            name=f\"length_constraint_{j}\"\n",
    "        )\n",
    "\n",
    "    # Constraint 4: Total active languages â€‹â€‹â‰¤ alpha\n",
    "    model.addConstr(quicksum(y[j] for j in languages) <= alpha)\n",
    "\n",
    "    # Objective: maximize total logit\n",
    "    model.setObjective(quicksum(x[(i, j)] * b[(i, j)] for (i, j) in x), GRB.MAXIMIZE)\n",
    "    model.optimize()\n",
    "\n",
    "    # Get assignment results\n",
    "    assignment = []\n",
    "    for (i, j) in x:\n",
    "        if x[(i, j)].X > 0.5:\n",
    "            word = i.split('_', 1)[1]\n",
    "            assignment.append((word, j))\n",
    "\n",
    "    open_assignments_te.append(assignment)\n",
    "    results.append(assignment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4ec1ab8-9ee5-4b5e-8456-858a33738d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_assignments[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "02b50071-a278-4eed-bcfb-ce4f95707319",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "# Convert True Labels to real sets\n",
    "if isinstance(df_te1.iloc[0][\"True Labels\"], str):\n",
    "    df_te1[\"True Labels\"] = df_te1[\"True Labels\"].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "20ae49cc-adc2-4d6f-a942-bf8a10dc237c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact Match (EM): 135 \n",
      "Partial Match (PM): 143\n",
      "False Positives (FP - CS only): 186\n"
     ]
    }
   ],
   "source": [
    "result_te1 = evaluate_masklid_predictions(df_te1, open_assignments_te)\n",
    "\n",
    "print(f\"Exact Match (EM): {result_te1['EM']} \")\n",
    "print(f\"Partial Match (PM): {result_te1['PM']}\")\n",
    "print(f\"False Positives (FP - CS only): {result_te1['FP']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e26aab-85f4-4af4-a135-57edff309bb7",
   "metadata": {},
   "source": [
    "GlotLID with Turkish-ENglish Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "297f30f7-4bde-4934-8663-479712571195",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from gurobipy import Model, GRB, quicksum\n",
    "\n",
    "# Konfigurasi\n",
    "min_len = 15  \n",
    "alpha = 2     \n",
    "\n",
    "results = []\n",
    "glot_assignments_te = []\n",
    "\n",
    "\n",
    "for idx, row in df_te2.iterrows():\n",
    "    text = row[\"Text\"]\n",
    "\n",
    "    \n",
    "    scores_dict = try2.compute_v_per_word(text)\n",
    "\n",
    "    \n",
    "    top_alpha_languages = set()\n",
    "    for word in scores_dict:\n",
    "        sorted_logits = sorted(scores_dict[word]['logits'], key=lambda x: -x[1])\n",
    "        top_alpha = [lang for lang, _ in sorted_logits[:alpha]]\n",
    "        top_alpha_languages.update(top_alpha)\n",
    "\n",
    "    words = list(scores_dict.keys())\n",
    "    languages = list(top_alpha_languages)\n",
    "\n",
    "    \n",
    "    b = {}\n",
    "    for i in words:\n",
    "        for j, score in scores_dict[i]['logits']:\n",
    "            if j in languages:\n",
    "                b[(i, j)] = score\n",
    "\n",
    "   \n",
    "    model = Model(\"MaskLID_ILP\")\n",
    "    model.setParam('OutputFlag', 0)\n",
    "\n",
    "    # Variabel utama\n",
    "    x = {(i, j): model.addVar(vtype=GRB.BINARY, name=f\"x_{i}_{j}\") for (i, j) in b}\n",
    "    y = {j: model.addVar(vtype=GRB.BINARY, name=f\"y_{j}\") for j in languages}\n",
    "    model.update()\n",
    "\n",
    "    \n",
    "    word_lengths = {w: len(w.split('_', 1)[1]) for w in words}\n",
    "\n",
    "    \n",
    "    for i in words:\n",
    "        model.addConstr(quicksum(x[(i, j)] for j in languages if (i, j) in x) == 1)\n",
    "\n",
    "   \n",
    "    for j in languages:\n",
    "        model.addConstr(quicksum(x[(i, j)] for i in words if (i, j) in x) <= len(words) * y[j])\n",
    "\n",
    "    \n",
    "    for j in languages:\n",
    "        model.addConstr(\n",
    "            quicksum(x[(i, j)] * word_lengths[i] for i in words if (i, j) in x) >= min_len * y[j],\n",
    "            name=f\"length_constraint_{j}\"\n",
    "        )\n",
    "\n",
    "   \n",
    "    model.addConstr(quicksum(y[j] for j in languages) <= alpha)\n",
    "\n",
    "    \n",
    "    model.setObjective(quicksum(x[(i, j)] * b[(i, j)] for (i, j) in x), GRB.MAXIMIZE)\n",
    "    model.optimize()\n",
    "\n",
    "    \n",
    "    assignment = []\n",
    "    for (i, j) in x:\n",
    "        if x[(i, j)].X > 0.5:\n",
    "            word = i.split('_', 1)[1]\n",
    "            assignment.append((word, j))\n",
    "\n",
    "    glot_assignments_te.append(assignment)\n",
    "    results.append(assignment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "51322817-2662-4581-a1cd-624be816469e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "\n",
    "if isinstance(df_te2.iloc[0][\"True Labels\"], str):\n",
    "    df_te2[\"True Labels\"] = df_te2[\"True Labels\"].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "502c3fbf-6d63-410d-bfed-3fa2fcf79ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact Match (EM): 127 \n",
      "Partial Match (PM): 133\n",
      "False Positives (FP - CS only): 199\n"
     ]
    }
   ],
   "source": [
    "result_te2 = evaluate_masklid_predictions(df_te1, glot_assignments_te)\n",
    "\n",
    "print(f\"Exact Match (EM): {result_te2['EM']} \")\n",
    "print(f\"Partial Match (PM): {result_te2['PM']}\")\n",
    "print(f\"False Positives (FP - CS only): {result_te2['FP']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacfdfab-4325-4b0e-a6f1-4129755a8f2f",
   "metadata": {},
   "source": [
    "OpenLID with Turkish Only Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "db773d1c-9d4a-4672-9b33-c06e3ba93029",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final\n",
    "from gurobipy import Model, GRB, quicksum\n",
    "\n",
    "# Konfigurasi\n",
    "min_len = 15  \n",
    "alpha = 2     \n",
    "\n",
    "results = []\n",
    "open_assignments_t = []\n",
    "\n",
    "\n",
    "for idx, row in df_t1.iterrows():\n",
    "    text = row[\"Text\"]\n",
    "\n",
    "    \n",
    "    scores_dict = try1.compute_v_per_word(text)\n",
    "\n",
    "    \n",
    "    top_alpha_languages = set()\n",
    "    for word in scores_dict:\n",
    "        sorted_logits = sorted(scores_dict[word]['logits'], key=lambda x: -x[1])\n",
    "        top_alpha = [lang for lang, _ in sorted_logits[:alpha]]\n",
    "        top_alpha_languages.update(top_alpha)\n",
    "\n",
    "    words = list(scores_dict.keys())\n",
    "    languages = list(top_alpha_languages)\n",
    "\n",
    "    \n",
    "    b = {}\n",
    "    for i in words:\n",
    "        for j, score in scores_dict[i]['logits']:\n",
    "            if j in languages:\n",
    "                b[(i, j)] = score\n",
    "\n",
    "   \n",
    "    model = Model(\"MaskLID_ILP\")\n",
    "    model.setParam('OutputFlag', 0)\n",
    "\n",
    "   \n",
    "    x = {(i, j): model.addVar(vtype=GRB.BINARY, name=f\"x_{i}_{j}\") for (i, j) in b}\n",
    "    y = {j: model.addVar(vtype=GRB.BINARY, name=f\"y_{j}\") for j in languages}\n",
    "    model.update()\n",
    "\n",
    "    \n",
    "    word_lengths = {w: len(w.split('_', 1)[1]) for w in words}\n",
    "\n",
    "   \n",
    "    for i in words:\n",
    "        model.addConstr(quicksum(x[(i, j)] for j in languages if (i, j) in x) == 1)\n",
    "\n",
    "   \n",
    "    for j in languages:\n",
    "        model.addConstr(quicksum(x[(i, j)] for i in words if (i, j) in x) <= len(words) * y[j])\n",
    "\n",
    "    \n",
    "    for j in languages:\n",
    "        model.addConstr(\n",
    "            quicksum(x[(i, j)] * word_lengths[i] for i in words if (i, j) in x) >= min_len * y[j],\n",
    "            name=f\"length_constraint_{j}\"\n",
    "        )\n",
    "\n",
    "    model.addConstr(quicksum(y[j] for j in languages) <= alpha)\n",
    "\n",
    "  \n",
    "    model.setObjective(quicksum(x[(i, j)] * b[(i, j)] for (i, j) in x), GRB.MAXIMIZE)\n",
    "    model.optimize()\n",
    "\n",
    "   \n",
    "    assignment = []\n",
    "    for (i, j) in x:\n",
    "        if x[(i, j)].X > 0.5:\n",
    "            word = i.split('_', 1)[1]\n",
    "            assignment.append((word, j))\n",
    "\n",
    "    open_assignments_t.append(assignment)\n",
    "    results.append(assignment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9f4ef034-1801-439d-8ffe-55f8321e37f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "\n",
    "if isinstance(df_t1.iloc[0][\"True Labels\"], str):\n",
    "    df_t1[\"True Labels\"] = df_t1[\"True Labels\"].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4f81c797-299c-4b1c-a81e-6e5a6b7a0535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact Match (EM): 107 \n",
      "Partial Match (PM): 332\n",
      "False Positives (FP - CS only): 0\n"
     ]
    }
   ],
   "source": [
    "result_t1 = evaluate_masklid_predictions(df_t1, open_assignments_t)\n",
    "\n",
    "print(f\"Exact Match (EM): {result_t1['EM']} \")\n",
    "print(f\"Partial Match (PM): {result_t1['PM']}\")\n",
    "print(f\"False Positives (FP - CS only): {result_t1['FP']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036d8093-6f27-4aa8-82c5-e7fcd197cc78",
   "metadata": {},
   "source": [
    "GlotLID with Turkish Only Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "257f111f-9bd7-487f-9488-59f1515f4cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final\n",
    "from gurobipy import Model, GRB, quicksum\n",
    "\n",
    "# Konfigurasi\n",
    "min_len = 15  \n",
    "alpha = 2     \n",
    "\n",
    "results = []\n",
    "glot_assignments_t = []\n",
    "\n",
    "\n",
    "for idx, row in df_t2.iterrows():\n",
    "    text = row[\"Text\"]\n",
    "\n",
    "   \n",
    "    scores_dict = try2.compute_v_per_word(text)\n",
    "\n",
    "  \n",
    "    top_alpha_languages = set()\n",
    "    for word in scores_dict:\n",
    "        sorted_logits = sorted(scores_dict[word]['logits'], key=lambda x: -x[1])\n",
    "        top_alpha = [lang for lang, _ in sorted_logits[:alpha]]\n",
    "        top_alpha_languages.update(top_alpha)\n",
    "\n",
    "    words = list(scores_dict.keys())\n",
    "    languages = list(top_alpha_languages)\n",
    "\n",
    "    \n",
    "    b = {}\n",
    "    for i in words:\n",
    "        for j, score in scores_dict[i]['logits']:\n",
    "            if j in languages:\n",
    "                b[(i, j)] = score\n",
    "\n",
    "  \n",
    "    model = Model(\"MaskLID_ILP\")\n",
    "    model.setParam('OutputFlag', 0)\n",
    "\n",
    "  \n",
    "    x = {(i, j): model.addVar(vtype=GRB.BINARY, name=f\"x_{i}_{j}\") for (i, j) in b}\n",
    "    y = {j: model.addVar(vtype=GRB.BINARY, name=f\"y_{j}\") for j in languages}\n",
    "    model.update()\n",
    "\n",
    "   \n",
    "    word_lengths = {w: len(w.split('_', 1)[1]) for w in words}\n",
    "\n",
    "  \n",
    "    for i in words:\n",
    "        model.addConstr(quicksum(x[(i, j)] for j in languages if (i, j) in x) == 1)\n",
    "\n",
    "  \n",
    "    for j in languages:\n",
    "        model.addConstr(quicksum(x[(i, j)] for i in words if (i, j) in x) <= len(words) * y[j])\n",
    "\n",
    "   \n",
    "    for j in languages:\n",
    "        model.addConstr(\n",
    "            quicksum(x[(i, j)] * word_lengths[i] for i in words if (i, j) in x) >= min_len * y[j],\n",
    "            name=f\"length_constraint_{j}\"\n",
    "        )\n",
    "\n",
    "   \n",
    "    model.addConstr(quicksum(y[j] for j in languages) <= alpha)\n",
    "\n",
    " \n",
    "    model.setObjective(quicksum(x[(i, j)] * b[(i, j)] for (i, j) in x), GRB.MAXIMIZE)\n",
    "    model.optimize()\n",
    "\n",
    "    assignment = []\n",
    "    for (i, j) in x:\n",
    "        if x[(i, j)].X > 0.5:\n",
    "            word = i.split('_', 1)[1]\n",
    "            assignment.append((word, j))\n",
    "\n",
    "    glot_assignments_t.append(assignment)\n",
    "    results.append(assignment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "00c5f64d-aa3a-49d9-a313-5c8564151463",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "# Convert True Labels jadi set beneran\n",
    "if isinstance(df_t2.iloc[0][\"True Labels\"], str):\n",
    "    df_t2[\"True Labels\"] = df_t2[\"True Labels\"].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a2d1586b-3449-4436-a41e-1f96a9a1a8c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact Match (EM): 109 \n",
      "Partial Match (PM): 332\n",
      "False Positives (FP - CS only): 0\n"
     ]
    }
   ],
   "source": [
    "result_t2 = evaluate_masklid_predictions(df_t2, glot_assignments_t)\n",
    "\n",
    "print(f\"Exact Match (EM): {result_t2['EM']} \")\n",
    "print(f\"Partial Match (PM): {result_t2['PM']}\")\n",
    "print(f\"False Positives (FP - CS only): {result_t2['FP']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
